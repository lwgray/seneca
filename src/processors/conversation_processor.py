"""
Local conversation processor for Seneca.

This module provides conversation processing functionality for Seneca,
reading directly from Marcus log files without requiring Marcus imports.
It replaces the direct dependency on Marcus's ConversationLogger.
"""

import json
import os
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
from enum import Enum


class ConversationType(Enum):
    """
    Enumeration of conversation types in the Marcus system.
    
    Attributes:
        WORKER_TO_PM: Communications from worker agents to PM agent
        PM_TO_WORKER: Communications from PM agent to worker agents  
        PM_TO_KANBAN: Communications from PM agent to kanban board
        KANBAN_TO_PM: Communications from kanban board to PM agent
        INTERNAL_THINKING: Internal reasoning processes
        DECISION: Formal decisions made by PM agent
        ERROR: Error conditions and exceptions
    """
    
    WORKER_TO_PM = "worker_to_pm"
    PM_TO_WORKER = "pm_to_worker"
    PM_TO_KANBAN = "pm_to_kanban"
    KANBAN_TO_PM = "kanban_to_pm"
    INTERNAL_THINKING = "internal_thinking"
    DECISION = "decision"
    ERROR = "error"


class ConversationProcessor:
    """
    Process conversation data from Marcus log files.
    
    This class provides methods to read and process conversation logs
    generated by Marcus, enabling Seneca to visualize agent communications
    without requiring direct imports from Marcus.
    
    Attributes:
        log_dir (Path): Directory containing Marcus conversation logs
        
    Example:
        >>> processor = ConversationProcessor("/path/to/marcus/logs")
        >>> recent = processor.get_recent_conversations(limit=50)
        >>> analytics = processor.get_conversation_analytics()
    """
    
    def __init__(self, log_dir: Union[str, Path]):
        """
        Initialize the conversation processor.
        
        Args:
            log_dir: Path to Marcus conversation log directory
            
        Raises:
            ValueError: If log directory doesn't exist
        """
        self.log_dir = Path(log_dir)
        if not self.log_dir.exists():
            raise ValueError(f"Log directory does not exist: {log_dir}")
    
    def get_recent_conversations(self, limit: int = 100) -> List[Dict[str, Any]]:
        """
        Get the most recent conversations from log files.
        
        Args:
            limit: Maximum number of conversations to return
            
        Returns:
            List of conversation dictionaries sorted by timestamp (newest first)
        """
        conversations = []
        
        # Find all JSONL log files
        log_files = list(self.log_dir.glob("*.jsonl"))
        
        # Sort by modification time to read newest files first
        log_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        
        # Read more conversations than requested to ensure we get the newest ones
        # We'll read up to 10x the limit or 1000, whichever is smaller
        read_limit = min(limit * 10, 1000)
        
        for log_file in log_files:
            if len(conversations) >= read_limit:
                break
                
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if len(conversations) >= read_limit:
                            break
                            
                        line = line.strip()
                        if line:
                            try:
                                record = json.loads(line)
                                conversations.append(record)
                            except json.JSONDecodeError:
                                # Skip invalid JSON lines
                                continue
            except (IOError, OSError) as e:
                # Log error but continue with other files
                print(f"Error reading {log_file}: {e}")
                continue
        
        # Sort by timestamp (newest first)
        conversations.sort(
            key=lambda x: x.get("timestamp", ""), 
            reverse=True
        )
        
        return conversations[:limit]
    
    def get_conversations_in_range(
        self,
        start_time: datetime,
        end_time: datetime,
        conversation_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Get conversations within a specific time range.
        
        Args:
            start_time: Start of time range
            end_time: End of time range
            conversation_type: Optional filter by conversation type
            
        Returns:
            List of conversations within the time range
        """
        conversations = []
        
        for log_file in self.log_dir.glob("*.jsonl"):
            # Skip files modified before our time range
            file_mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
            # Make file_mtime timezone-aware if start_time is
            if start_time.tzinfo is not None and file_mtime.tzinfo is None:
                file_mtime = file_mtime.replace(tzinfo=timezone.utc)
            if file_mtime < start_time:
                continue
                
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if not line:
                            continue
                            
                        try:
                            record = json.loads(line)
                            
                            # Parse timestamp
                            timestamp_str = record.get("timestamp", "")
                            if not timestamp_str:
                                continue
                                
                            try:
                                timestamp = datetime.fromisoformat(
                                    timestamp_str.replace('Z', '+00:00')
                                )
                            except ValueError:
                                continue
                            
                            # Make datetime comparisons timezone-aware
                            # If start_time/end_time are naive, make them UTC
                            if start_time.tzinfo is None:
                                start_time = start_time.replace(tzinfo=timezone.utc)
                            if end_time.tzinfo is None:
                                end_time = end_time.replace(tzinfo=timezone.utc)
                            
                            # Check time range
                            if start_time <= timestamp <= end_time:
                                # Apply type filter if specified
                                if conversation_type:
                                    if record.get("type") == conversation_type:
                                        conversations.append(record)
                                else:
                                    conversations.append(record)
                                    
                        except json.JSONDecodeError:
                            continue
                            
            except (IOError, OSError) as e:
                print(f"Error reading {log_file}: {e}")
                continue
        
        return conversations
    
    def get_agent_conversations(
        self, 
        agent_id: str, 
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """
        Get conversations for a specific agent.
        
        Args:
            agent_id: ID of the agent to filter by
            limit: Maximum number of conversations to return
            
        Returns:
            List of conversations involving the specified agent
        """
        agent_convs = []
        
        # Get recent conversations and filter
        all_convs = self.get_recent_conversations(limit=limit * 5)  # Get more to filter
        
        for conv in all_convs:
            if len(agent_convs) >= limit:
                break
                
            # Check if agent is involved
            if (conv.get("source") == agent_id or 
                conv.get("target") == agent_id or
                conv.get("worker_id") == agent_id or
                conv.get("agent_id") == agent_id):
                agent_convs.append(conv)
        
        return agent_convs
    
    def get_conversation_analytics(
        self, 
        hours: int = 24
    ) -> Dict[str, Any]:
        """
        Calculate analytics for conversations over a time period.
        
        Args:
            hours: Number of hours to analyze
            
        Returns:
            Dictionary containing various analytics metrics
        """
        end_time = datetime.now(timezone.utc)
        start_time = end_time - timedelta(hours=hours)
        
        conversations = self.get_conversations_in_range(start_time, end_time)
        
        # Calculate various metrics
        analytics = {
            "total_conversations": len(conversations),
            "conversations_by_type": self._count_by_type(conversations),
            "active_agents": self._count_active_agents(conversations),
            "average_confidence": self._calculate_avg_confidence(conversations),
            "blockers": self._count_blockers(conversations),
            "time_range": {
                "start": start_time.isoformat(),
                "end": end_time.isoformat(),
                "hours": hours
            }
        }
        
        return analytics
    
    def _count_by_type(self, conversations: List[Dict[str, Any]]) -> Dict[str, int]:
        """Count conversations by type."""
        type_counts = {}
        for conv in conversations:
            conv_type = conv.get("type", conv.get("conversation_type", "unknown"))
            type_counts[conv_type] = type_counts.get(conv_type, 0) + 1
        return type_counts
    
    def _count_active_agents(self, conversations: List[Dict[str, Any]]) -> int:
        """Count unique active agents."""
        agents = set()
        for conv in conversations:
            if conv.get("source", "").startswith("worker"):
                agents.add(conv["source"])
            if conv.get("worker_id"):
                agents.add(conv["worker_id"])
            if conv.get("agent_id"):
                agents.add(conv["agent_id"])
        return len(agents)
    
    def _calculate_avg_confidence(self, conversations: List[Dict[str, Any]]) -> float:
        """Calculate average confidence for decisions."""
        confidences = []
        for conv in conversations:
            if conv.get("type") == "decision" or conv.get("type") == "pm_decision":
                confidence = conv.get("confidence_score")
                if confidence is None:
                    # Try metadata
                    confidence = conv.get("metadata", {}).get("confidence_score")
                if confidence is not None:
                    confidences.append(float(confidence))
        
        return sum(confidences) / len(confidences) if confidences else 0.0
    
    def _count_blockers(self, conversations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Count and analyze blockers."""
        blockers = []
        for conv in conversations:
            if "blocker" in conv.get("type", "").lower() or \
               "blocker" in conv.get("event_type", "").lower():
                blockers.append(conv)
        
        severity_counts = {}
        for blocker in blockers:
            severity = blocker.get("severity", "medium")
            if not severity:
                severity = blocker.get("metadata", {}).get("severity", "medium")
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
        
        return {
            "total": len(blockers),
            "by_severity": severity_counts
        }


class ConversationStreamProcessor:
    """
    Process real-time conversation streams.
    
    This class monitors log files for new conversations and provides
    real-time streaming capabilities for the Seneca UI.
    
    Attributes:
        log_dir (Path): Directory to monitor for new log entries
        processor (ConversationProcessor): Processor for reading logs
    """
    
    def __init__(self, log_dir: Union[str, Path]):
        """
        Initialize the stream processor.
        
        Args:
            log_dir: Path to Marcus conversation log directory
        """
        self.log_dir = Path(log_dir)
        self.processor = ConversationProcessor(log_dir)
        self._last_read_positions = {}  # Track file positions
    
    def get_new_conversations(self) -> List[Dict[str, Any]]:
        """
        Get new conversations since last check.
        
        Returns:
            List of new conversation records
        """
        new_conversations = []
        
        # Find the most recent log file
        log_files = list(self.log_dir.glob("*.jsonl"))
        if not log_files:
            return []
        
        # Get the newest file
        newest_file = max(log_files, key=lambda f: f.stat().st_mtime)
        
        # Read from last position
        last_pos = self._last_read_positions.get(str(newest_file), 0)
        
        try:
            with open(newest_file, 'r', encoding='utf-8') as f:
                f.seek(last_pos)
                
                for line in f:
                    line = line.strip()
                    if line:
                        try:
                            record = json.loads(line)
                            new_conversations.append(record)
                        except json.JSONDecodeError:
                            continue
                
                # Update position
                self._last_read_positions[str(newest_file)] = f.tell()
                
        except (IOError, OSError) as e:
            print(f"Error reading {newest_file}: {e}")
        
        return new_conversations